{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3aOPnpfDheF1W5dF0cr2R"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-nBNsm6n_Rv"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BPE -Byte Pair Encoding**"
      ],
      "metadata": {
        "id": "w5n6fWQGoH4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"This is the Hugging Face Course.\",\n",
        "    \"This chapter is about tokenization.\",\n",
        "    \"This section shows several tokenizer algorithms.\",\n",
        "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
        "]"
      ],
      "metadata": {
        "id": "a0DLZUZToBJ1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-tokenization using gpt2 which uses BPE for tokenization\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "UagbEktkoZya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0df85f0-887f-4e32-dbd7-16444a455507"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a95de0416b942b6ad2848cc9b8edee5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46af48f8994e4a2c82ccf3d3a6c0a153"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "076faa640a3d4484adfb8add7f15bcd3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "852120dafc564273bfae1cc981e25046"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the frequencies of each word\n",
        "from collections import defaultdict\n",
        "\n",
        "word_freqs = defaultdict(int)\n",
        "\n",
        "for text in corpus:\n",
        "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    new_words = [word for word, offset in words_with_offsets]\n",
        "    for word in new_words:\n",
        "        word_freqs[word] += 1\n",
        "\n",
        "print(word_freqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6BQDnieop-j",
        "outputId": "a2702574-3c50-435a-f8fe-0188a0190a9c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>, {'This': 3, 'Ġis': 2, 'Ġthe': 1, 'ĠHugging': 1, 'ĠFace': 1, 'ĠCourse': 1, '.': 4, 'Ġchapter': 1, 'Ġabout': 1, 'Ġtokenization': 1, 'Ġsection': 1, 'Ġshows': 1, 'Ġseveral': 1, 'Ġtokenizer': 1, 'Ġalgorithms': 1, 'Hopefully': 1, ',': 1, 'Ġyou': 1, 'Ġwill': 1, 'Ġbe': 1, 'Ġable': 1, 'Ġto': 1, 'Ġunderstand': 1, 'Ġhow': 1, 'Ġthey': 1, 'Ġare': 1, 'Ġtrained': 1, 'Ġand': 1, 'Ġgenerate': 1, 'Ġtokens': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the base vocabulary (all characters used)\n",
        "alphabet = []\n",
        "\n",
        "for word in word_freqs.keys():\n",
        "    for letter in word:\n",
        "        if letter not in alphabet:\n",
        "            alphabet.append(letter)\n",
        "alphabet.sort()\n",
        "\n",
        "print(alphabet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9m-JaZDo9rI",
        "outputId": "68c030ec-1305-40dc-96f4-6982c45a4fa2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We also add the special tokens used by the model at the beginning of that vocabulary. In the case of GPT-2, the only special token is \"<|endoftext|>\"\n",
        "vocab = [\"<|endoftext|>\"] + alphabet.copy()"
      ],
      "metadata": {
        "id": "1mAcrowDpJyh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split each word into individual characters\n",
        "splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
        "splits[\"This\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoxUHt-JpUfp",
        "outputId": "56aea424-786a-4f5b-b209-60e6fbe5cf77"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T', 'h', 'i', 's']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# computes the frequency of each pair.\n",
        "def compute_pair_freqs(splits):\n",
        "    pair_freqs = defaultdict(int)\n",
        "    for word, freq in word_freqs.items():\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "        for i in range(len(split) - 1):\n",
        "            pair = (split[i], split[i + 1])\n",
        "            pair_freqs[pair] += freq\n",
        "    return pair_freqs"
      ],
      "metadata": {
        "id": "JScHXDt_pf5_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pair_freqs = compute_pair_freqs(splits)\n",
        "\n",
        "#look at some of them\n",
        "for i, key in enumerate(pair_freqs.keys()):\n",
        "    print(f\"{key}: {pair_freqs[key]}\")\n",
        "    if i >= 5:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTY_21cyp2T7",
        "outputId": "4a22864c-ee37-4d70-a1b8-fcb953ad3b11"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('T', 'h'): 3\n",
            "('h', 'i'): 3\n",
            "('i', 's'): 5\n",
            "('Ġ', 'i'): 2\n",
            "('Ġ', 't'): 7\n",
            "('t', 'h'): 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the most frequent pair\n",
        "best_pair = \"\"\n",
        "max_freq = None\n",
        "\n",
        "for pair, freq in pair_freqs.items():\n",
        "    if max_freq is None or max_freq < freq:\n",
        "        best_pair = pair\n",
        "        max_freq = freq\n",
        "\n",
        "print(best_pair, max_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVnkYFDNqGbz",
        "outputId": "f8ed3462-a316-48c4-8cfa-487464bc4376"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Ġ', 't') 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merges = {(\"Ġ\", \"t\"): \"Ġt\"}\n",
        "vocab.append(\"Ġt\")"
      ],
      "metadata": {
        "id": "25vAfjsJqXzc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_pair(a, b, splits):\n",
        "    for word in word_freqs:\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "\n",
        "        i = 0\n",
        "        while i < len(split) - 1:\n",
        "            if split[i] == a and split[i + 1] == b:\n",
        "                split = split[:i] + [a + b] + split[i + 2 :]\n",
        "            else:\n",
        "                i += 1\n",
        "        splits[word] = split\n",
        "    return splits"
      ],
      "metadata": {
        "id": "GLacy3H3qejr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits = merge_pair(\"Ġ\", \"t\", splits)\n",
        "print(splits[\"Ġtrained\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rc9EIVRequ19",
        "outputId": "694a793f-9593-4fa1-af2a-0ec4cdec5c35"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ġt', 'r', 'a', 'i', 'n', 'e', 'd']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For a vocab size of 50\n",
        "vocab_size = 50\n",
        "\n",
        "while len(vocab) < vocab_size:\n",
        "    pair_freqs = compute_pair_freqs(splits)\n",
        "    best_pair = \"\"\n",
        "    max_freq = None\n",
        "    for pair, freq in pair_freqs.items():\n",
        "        if max_freq is None or max_freq < freq:\n",
        "            best_pair = pair\n",
        "            max_freq = freq\n",
        "    splits = merge_pair(*best_pair, splits)\n",
        "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
        "    vocab.append(best_pair[0] + best_pair[1])"
      ],
      "metadata": {
        "id": "jcsoICMwqx2x"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(merges)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9bGD2JErQmQ",
        "outputId": "4ae36487-10a9-421e-9636-4bf64ecc391f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('Ġ', 't'): 'Ġt', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ġ', 'a'): 'Ġa', ('Ġt', 'o'): 'Ġto', ('e', 'n'): 'en', ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ġto', 'k'): 'Ġtok', ('Ġtok', 'en'): 'Ġtoken', ('n', 'd'): 'nd', ('Ġ', 'is'): 'Ġis', ('Ġt', 'h'): 'Ġth', ('Ġth', 'e'): 'Ġthe', ('i', 'n'): 'in', ('Ġa', 'b'): 'Ġab', ('Ġtoken', 'i'): 'Ġtokeni'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the vocabulary is composed of the special token, the initial alphabet, and all the results of the merges\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdZP04r0rpW4",
        "outputId": "fe46caa8-5ff0-4167-b6de-cf99459d0bd5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ', 'Ġt', 'is', 'er', 'Ġa', 'Ġto', 'en', 'Th', 'This', 'ou', 'se', 'Ġtok', 'Ġtoken', 'nd', 'Ġis', 'Ġth', 'Ġthe', 'in', 'Ġab', 'Ġtokeni']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize a new text\n",
        "def tokenize(text):\n",
        "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
        "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
        "    for pair, merge in merges.items():\n",
        "        for idx, split in enumerate(splits):\n",
        "            i = 0\n",
        "            while i < len(split) - 1:\n",
        "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
        "                    split = split[:i] + [merge] + split[i + 2 :]\n",
        "                else:\n",
        "                    i += 1\n",
        "            splits[idx] = split\n",
        "\n",
        "    return sum(splits, [])"
      ],
      "metadata": {
        "id": "jL6Gsp7Hr0ax"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "tokenize(\"This is not a token.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8r7SSbWsEAL",
        "outputId": "3f762be5-dcb2-4a2b-e701-fd3fa178e95a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'Ġis', 'Ġ', 'n', 'o', 't', 'Ġa', 'Ġtoken', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize(\"Hugging Face Course.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B016jq2XsXAy",
        "outputId": "221fb844-7ccf-4762-cf18-eaa3dff76ad0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['H',\n",
              " 'u',\n",
              " 'g',\n",
              " 'g',\n",
              " 'in',\n",
              " 'g',\n",
              " 'Ġ',\n",
              " 'F',\n",
              " 'a',\n",
              " 'c',\n",
              " 'e',\n",
              " 'Ġ',\n",
              " 'C',\n",
              " 'ou',\n",
              " 'r',\n",
              " 'se',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **WordPiece tokenization**"
      ],
      "metadata": {
        "id": "owwoyW8htUuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "RX8KZx5gtHaw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22b09ee1-b68e-4be6-e8d4-ff39d9997a4e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9de75006dce049858e30eaa50af1899d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68254e049bac4d1c8e0ede7e68827098"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5440ef384044d2885d1c17c80d5fd97"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "020f4ab6d9544e71aeae77691f61aa78"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ompute the frequencies of each word\n",
        "from collections import defaultdict\n",
        "\n",
        "word_freqs = defaultdict(int)\n",
        "for text in corpus:\n",
        "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    new_words = [word for word, offset in words_with_offsets]\n",
        "    for word in new_words:\n",
        "        word_freqs[word] += 1\n",
        "\n",
        "word_freqs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzvv9KtcNqna",
        "outputId": "9117ae76-5828-4386-9347-494abed7fda3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'This': 3,\n",
              "             'is': 2,\n",
              "             'the': 1,\n",
              "             'Hugging': 1,\n",
              "             'Face': 1,\n",
              "             'Course': 1,\n",
              "             '.': 4,\n",
              "             'chapter': 1,\n",
              "             'about': 1,\n",
              "             'tokenization': 1,\n",
              "             'section': 1,\n",
              "             'shows': 1,\n",
              "             'several': 1,\n",
              "             'tokenizer': 1,\n",
              "             'algorithms': 1,\n",
              "             'Hopefully': 1,\n",
              "             ',': 1,\n",
              "             'you': 1,\n",
              "             'will': 1,\n",
              "             'be': 1,\n",
              "             'able': 1,\n",
              "             'to': 1,\n",
              "             'understand': 1,\n",
              "             'how': 1,\n",
              "             'they': 1,\n",
              "             'are': 1,\n",
              "             'trained': 1,\n",
              "             'and': 1,\n",
              "             'generate': 1,\n",
              "             'tokens': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#alphabet\n",
        "alphabet = []\n",
        "for word in word_freqs.keys():\n",
        "    if word[0] not in alphabet:\n",
        "        alphabet.append(word[0])\n",
        "    for letter in word[1:]:\n",
        "        if f\"##{letter}\" not in alphabet:\n",
        "            alphabet.append(f\"##{letter}\")\n",
        "\n",
        "alphabet.sort()\n",
        "alphabet\n",
        "\n",
        "print(alphabet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvoOV7RrN5eV",
        "outputId": "3fe3a094-80ca-466a-a7d0-e13a372e46b6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add the special tokens\n",
        "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()"
      ],
      "metadata": {
        "id": "ohFSisbJOOkC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  split each word\n",
        "splits = {\n",
        "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
        "    for word in word_freqs.keys()\n",
        "}"
      ],
      "metadata": {
        "id": "2v8hbZpUOZZR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computes the score of each pair\n",
        "def compute_pair_scores(splits):\n",
        "    letter_freqs = defaultdict(int)\n",
        "    pair_freqs = defaultdict(int)\n",
        "    for word, freq in word_freqs.items():\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            letter_freqs[split[0]] += freq\n",
        "            continue\n",
        "        for i in range(len(split) - 1):\n",
        "            pair = (split[i], split[i + 1])\n",
        "            letter_freqs[split[i]] += freq\n",
        "            pair_freqs[pair] += freq\n",
        "        letter_freqs[split[-1]] += freq\n",
        "\n",
        "    scores = {\n",
        "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
        "        for pair, freq in pair_freqs.items()\n",
        "    }\n",
        "    return scores"
      ],
      "metadata": {
        "id": "uy5s5GE9Olly"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pair_scores = compute_pair_scores(splits)\n",
        "for i, key in enumerate(pair_scores.keys()):\n",
        "    print(f\"{key}: {pair_scores[key]}\")\n",
        "    if i >= 5:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxrs3825PNYg",
        "outputId": "d13e6f75-801f-48e2-c164-cec7dcefbf74"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('T', '##h'): 0.125\n",
            "('##h', '##i'): 0.03409090909090909\n",
            "('##i', '##s'): 0.02727272727272727\n",
            "('i', '##s'): 0.1\n",
            "('t', '##h'): 0.03571428571428571\n",
            "('##h', '##e'): 0.011904761904761904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find the highest freq\n",
        "best_pair = \"\"\n",
        "max_score = None\n",
        "for pair, score in pair_scores.items():\n",
        "    if max_score is None or max_score < score:\n",
        "        best_pair = pair\n",
        "        max_score = score\n",
        "\n",
        "print(best_pair, max_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tL5PAgBxRFQk",
        "outputId": "0ef04735-1aa8-48eb-e448-90bf2fbf43a9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('a', '##b') 0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.append(\"ab\")"
      ],
      "metadata": {
        "id": "juhPcY8JRf-_"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_pair(a, b, splits):\n",
        "    for word in word_freqs:\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "        i = 0\n",
        "        while i < len(split) - 1:\n",
        "            if split[i] == a and split[i + 1] == b:\n",
        "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
        "                split = split[:i] + [merge] + split[i + 2 :]\n",
        "            else:\n",
        "                i += 1\n",
        "        splits[word] = split\n",
        "    return splits"
      ],
      "metadata": {
        "id": "Q7rtOFmYRk9p"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits = merge_pair(\"a\", \"##b\", splits)\n",
        "splits[\"about\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn9kXzwTRn7e",
        "outputId": "3260eadb-a3df-4915-e3d1-96593fc62f66"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ab', '##o', '##u', '##t']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 70\n",
        "while len(vocab) < vocab_size:\n",
        "    scores = compute_pair_scores(splits)\n",
        "    best_pair, max_score = \"\", None\n",
        "    for pair, score in scores.items():\n",
        "        if max_score is None or max_score < score:\n",
        "            best_pair = pair\n",
        "            max_score = score\n",
        "    splits = merge_pair(*best_pair, splits)\n",
        "    new_token = (\n",
        "        best_pair[0] + best_pair[1][2:]\n",
        "        if best_pair[1].startswith(\"##\")\n",
        "        else best_pair[0] + best_pair[1]\n",
        "    )\n",
        "    vocab.append(new_token)"
      ],
      "metadata": {
        "id": "KldHx9D8RwWV"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_bVgqfYSElT",
        "outputId": "0ccd2ddc-d02f-4f75-8cdd-3f13b73de915"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat', '##ut']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_word(word):\n",
        "    tokens = []\n",
        "    while len(word) > 0:\n",
        "        i = len(word)\n",
        "        while i > 0 and word[:i] not in vocab:\n",
        "            i -= 1\n",
        "        if i == 0:\n",
        "            return [\"[UNK]\"]\n",
        "        tokens.append(word[:i])\n",
        "        word = word[i:]\n",
        "        if len(word) > 0:\n",
        "            word = f\"##{word}\"\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "lOogUXA1SHFp"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode_word(\"Hugging\"))\n",
        "print(encode_word(\"HOgging\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IerPc3LSGNa",
        "outputId": "44f71b55-90dc-438d-8176-f1f744094897"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hugg', '##i', '##n', '##g']\n",
            "['[UNK]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
        "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
        "    return sum(encoded_words, [])"
      ],
      "metadata": {
        "id": "UU5JGS_fSx0g"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize(\"This is the Hugging Face course!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqdO4YvQTgqy",
        "outputId": "6f4c5fcb-a88a-43a5-b396-b97388bed4ab"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Th',\n",
              " '##i',\n",
              " '##s',\n",
              " 'is',\n",
              " 'th',\n",
              " '##e',\n",
              " 'Hugg',\n",
              " '##i',\n",
              " '##n',\n",
              " '##g',\n",
              " 'Fac',\n",
              " '##e',\n",
              " 'c',\n",
              " '##o',\n",
              " '##u',\n",
              " '##r',\n",
              " '##s',\n",
              " '##e',\n",
              " '[UNK]']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Unigram tokenization**"
      ],
      "metadata": {
        "id": "A2Pi-SO0TpYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VSzJi2KTiSP",
        "outputId": "d57e7734-adf9-4ab5-811f-e9e4fb952b03"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbde645dc78a4eb8a0537232b0d3a349"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8149736b08d4639866b306b1e72dcb9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.38M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0546fc7886514382a3bc2997c6e19062"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "word_freqs = defaultdict(int)\n",
        "for text in corpus:\n",
        "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    new_words = [word for word, offset in words_with_offsets]\n",
        "    for word in new_words:\n",
        "        word_freqs[word] += 1\n",
        "\n",
        "word_freqs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoN01LmstvtG",
        "outputId": "597d4d5c-d216-42af-c106-b50465238ded"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'▁This': 3,\n",
              "             '▁is': 2,\n",
              "             '▁the': 1,\n",
              "             '▁Hugging': 1,\n",
              "             '▁Face': 1,\n",
              "             '▁Course.': 1,\n",
              "             '▁chapter': 1,\n",
              "             '▁about': 1,\n",
              "             '▁tokenization.': 1,\n",
              "             '▁section': 1,\n",
              "             '▁shows': 1,\n",
              "             '▁several': 1,\n",
              "             '▁tokenizer': 1,\n",
              "             '▁algorithms.': 1,\n",
              "             '▁Hopefully,': 1,\n",
              "             '▁you': 1,\n",
              "             '▁will': 1,\n",
              "             '▁be': 1,\n",
              "             '▁able': 1,\n",
              "             '▁to': 1,\n",
              "             '▁understand': 1,\n",
              "             '▁how': 1,\n",
              "             '▁they': 1,\n",
              "             '▁are': 1,\n",
              "             '▁trained': 1,\n",
              "             '▁and': 1,\n",
              "             '▁generate': 1,\n",
              "             '▁tokens.': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize our vocab\n",
        "char_freqs = defaultdict(int)\n",
        "subwords_freqs = defaultdict(int)\n",
        "for word, freq in word_freqs.items():\n",
        "    for i in range(len(word)):\n",
        "        char_freqs[word[i]] += freq\n",
        "        # Loop through the subwords of length at least 2\n",
        "        for j in range(i + 2, len(word) + 1):\n",
        "            subwords_freqs[word[i:j]] += freq\n",
        "\n",
        "# Sort subwords by frequency\n",
        "sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_subwords[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idL6lw9_ty-A",
        "outputId": "5f759dcc-0d3c-4dc9-a9cc-50ca0fcba0b9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('▁t', 7),\n",
              " ('is', 5),\n",
              " ('er', 5),\n",
              " ('▁a', 5),\n",
              " ('▁to', 4),\n",
              " ('to', 4),\n",
              " ('en', 4),\n",
              " ('▁T', 3),\n",
              " ('▁Th', 3),\n",
              " ('▁Thi', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# group the characters with the best subwords\n",
        "token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]\n",
        "token_freqs = {token: freq for token, freq in token_freqs}"
      ],
      "metadata": {
        "id": "bL67W2LQuNx5"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the freq and store their log for better computation\n",
        "from math import log\n",
        "\n",
        "total_sum = sum([freq for token, freq in token_freqs.items()])\n",
        "model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
      ],
      "metadata": {
        "id": "Pqx5Jqajur0z"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  tokenizes words using the Viterbi algorithm\n",
        "def encode_word(word, model):\n",
        "    best_segmentations = [{\"start\": 0, \"score\": 1}] + [\n",
        "        {\"start\": None, \"score\": None} for _ in range(len(word))\n",
        "    ]\n",
        "    for start_idx in range(len(word)):\n",
        "        # This should be properly filled by the previous steps of the loop\n",
        "        best_score_at_start = best_segmentations[start_idx][\"score\"]\n",
        "        for end_idx in range(start_idx + 1, len(word) + 1):\n",
        "            token = word[start_idx:end_idx]\n",
        "            if token in model and best_score_at_start is not None:\n",
        "                score = model[token] + best_score_at_start\n",
        "                # If we have found a better segmentation ending at end_idx, we update\n",
        "                if (\n",
        "                    best_segmentations[end_idx][\"score\"] is None\n",
        "                    or best_segmentations[end_idx][\"score\"] > score\n",
        "                ):\n",
        "                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n",
        "\n",
        "    segmentation = best_segmentations[-1]\n",
        "    if segmentation[\"score\"] is None:\n",
        "        # We did not find a tokenization of the word -> unknown\n",
        "        return [\"<unk>\"], None\n",
        "\n",
        "    score = segmentation[\"score\"]\n",
        "    start = segmentation[\"start\"]\n",
        "    end = len(word)\n",
        "    tokens = []\n",
        "    while start != 0:\n",
        "        tokens.insert(0, word[start:end])\n",
        "        next_start = best_segmentations[start][\"start\"]\n",
        "        end = start\n",
        "        start = next_start\n",
        "    tokens.insert(0, word[start:end])\n",
        "    return tokens, score"
      ],
      "metadata": {
        "id": "--LkNvZPu6fi"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode_word(\"Hopefully\", model))\n",
        "print(encode_word(\"This\", model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRuDlBzyv-L5",
        "outputId": "92fa8549-23e5-4a77-da6e-10d8e73592e2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)\n",
            "(['This'], 6.288267030694535)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(model):\n",
        "    loss = 0\n",
        "    for word, freq in word_freqs.items():\n",
        "        _, word_loss = encode_word(word, model)\n",
        "        loss += freq * word_loss\n",
        "    return loss"
      ],
      "metadata": {
        "id": "is1bEl6bwCPF"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_loss(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfuLEH_9wMBM",
        "outputId": "9de582a4-e1d5-4ef2-945f-96826fec60ea"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "413.10377642940875"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "\n",
        "def compute_scores(model):\n",
        "    scores = {}\n",
        "    model_loss = compute_loss(model)\n",
        "    for token, score in model.items():\n",
        "        # We always keep tokens of length 1\n",
        "        if len(token) == 1:\n",
        "            continue\n",
        "        model_without_token = copy.deepcopy(model)\n",
        "        _ = model_without_token.pop(token)\n",
        "        scores[token] = compute_loss(model_without_token) - model_loss\n",
        "    return scores"
      ],
      "metadata": {
        "id": "blNX4wPVwNd4"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = compute_scores(model)\n",
        "print(scores[\"ll\"])\n",
        "print(scores[\"his\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bj6VxNHLwV2G",
        "outputId": "c44b208b-d4ab-48e6-963d-c268228ba6d4"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.376412403623874\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percent_to_remove = 0.1\n",
        "while len(model) > 100:\n",
        "    scores = compute_scores(model)\n",
        "    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n",
        "    # Remove percent_to_remove tokens with the lowest scores.\n",
        "    for i in range(int(len(model) * percent_to_remove)):\n",
        "        _ = token_freqs.pop(sorted_scores[i][0])\n",
        "\n",
        "    total_sum = sum([freq for token, freq in token_freqs.items()])\n",
        "    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
      ],
      "metadata": {
        "id": "BAWIa6a2wZo7"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text, model):\n",
        "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "    pre_tokenized_text = [word for word, offset in words_with_offsets]\n",
        "    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]\n",
        "    return sum(encoded_words, [])\n",
        "\n",
        "\n",
        "tokenize(\"This is the Hugging Face course.\", model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InGpuPnpwrv9",
        "outputId": "9bc1fc9f-f0d9-4a9c-9d41-b16e87340398"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁This',\n",
              " '▁is',\n",
              " '▁the',\n",
              " '▁Hugging',\n",
              " '▁Face',\n",
              " '▁',\n",
              " 'c',\n",
              " 'ou',\n",
              " 'r',\n",
              " 's',\n",
              " 'e',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M3TwL_fEw0wA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}